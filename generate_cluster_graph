import math
import json
from collections import Counter, defaultdict
import random
import heapq

def haversine(lon1, lat1, lon2, lat2):
    R = 3959.87433 # Radius of Earth in miles

    dLat = math.radians(lat2 - lat1)
    dLon = math.radians(lon2 - lon1)
    lat1 = math.radians(lat1)
    lat2 = math.radians(lat2)

    a = math.sin(dLat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dLon/2)**2
    c = 2*math.atan2(math.sqrt(a), math.sqrt(1-a))
    distance = R * c
    return distance


def initialize_cluster_centers(node_data):
    num_rows = 10  # Adjust the number of rows as needed
    num_cols = 10  # Adjust the number of columns as needed

    min_lat = min(node['lat'] for node in node_data.values())
    max_lat = max(node['lat'] for node in node_data.values())
    min_lon = min(node['lon'] for node in node_data.values())
    max_lon = max(node['lon'] for node in node_data.values())

    lat_step = (max_lat - min_lat) / num_rows
    lon_step = (max_lon - min_lon) / num_cols

    grid = [[[] for _ in range(num_cols)] for _ in range(num_rows)]

    # Assign nodes to grid cells
    for node_id, node in node_data.items():
        row = min(int((node['lat'] - min_lat) / lat_step), num_rows - 1)
        col = min(int((node['lon'] - min_lon) / lon_step), num_cols - 1)
        grid[row][col].append(node_id)

    # Select nodes
    selected_nodes = []
    total_nodes_needed = 100

    # First, select one node from each cell if available
    for row in grid:
        for cell in row:
            if cell:
                selected_nodes.append(cell[0])
                total_nodes_needed -= 1

    # Randomly select additional nodes from populated cells if more nodes are needed
    while total_nodes_needed > 0:
        for row in grid:
            for cell in row:
                if total_nodes_needed <= 0:
                    break
                if len(cell) > 1:
                    random.shuffle(cell)
                    for node_id in cell[1:]:  # Starting from the second element to avoid duplicates
                        if total_nodes_needed > 0 and node_id not in selected_nodes:
                            selected_nodes.append(node_id)
                            total_nodes_needed -= 1
                        else:
                            break

    return selected_nodes




def assign_to_nearest_cluster(node, clusters):
    min_distance = float('inf')
    assigned_cluster = None

    for cluster_id, cluster in clusters.items():
        cluster_center = cluster['center']
        distance = haversine(node['lon'], node['lat'], cluster_center['lon'], cluster_center['lat'])

        if distance < min_distance:
            min_distance = distance
            assigned_cluster = cluster_id

    return assigned_cluster

def assign_nodes_to_clusters(nodes, initial_cluster_centers, graph):
    clusters = {}
    # Initialize clusters'
    for c in initial_cluster_centers:
        c_id = list(c.keys())[0]
        c_vals = list(c.values())[0]
        clusters[c_id] = {'center':c_vals}
        clusters[c_id]['members'] = []

    # Assign each node to the nearest cluster center
    for node_id, node in nodes.items():
        assigned_cluster = assign_to_nearest_cluster(node, clusters)
        clusters[assigned_cluster]['members'].append(node_id)

    return clusters


def test_all_nodes_assigned(nodes, clusters):
    unassigned_nodes = set(nodes.keys())
    
    for cluster in clusters.values():
        unassigned_nodes -= cluster['members']

    if not unassigned_nodes:
        print("All nodes have been successfully assigned to a cluster.")
    else:
        print("Some nodes are not assigned to any cluster:")
        for node_id in unassigned_nodes:
            print(f"Unassigned Node ID: {node_id}")



def average_cluster_connections(cluster_connections):
    averaged_connections = {}

    for cluster_id, connections in cluster_connections.items():
        averaged_connections[cluster_id] = {}
        for connected_cluster_id, conn_list in connections.items():
            times = defaultdict(list)
            for day_type, hour, time in conn_list:
                times[(day_type, hour)].append(time)

            # Calculate averages
            averaged_connections[cluster_id][connected_cluster_id] = {
                key: sum(time_list) / len(time_list) for key, time_list in times.items()
            }

    return averaged_connections

def aggregate_cluster_connections(nodes, clusters):
    cluster_connections = {cluster_id: defaultdict(list) for cluster_id in clusters.keys()}

    for cluster_id, cluster in clusters.items():
        for node_id in cluster['members']:
            node = nodes[node_id]
            for connected_node_id, connections in node['connections'].items():
                connected_cluster_id = find_node_cluster(connected_node_id, clusters)
                if connected_cluster_id != cluster_id:
                    for conn in connections:
                        cluster_connections[cluster_id][connected_cluster_id].append(
                            (conn['day_type'], conn['hour'], conn['time'])
                        )

    return cluster_connections

def find_node_cluster(node_id, clusters):
    for cluster_id, cluster in clusters.items():
        if node_id in cluster['members']:
            return cluster_id
    return None

def add_cluster_data_to_nodes(tiny_graph, clusters):
    graph_with_connections = {id:{} for id in tiny_graph.keys()}
    for node_id, data in tiny_graph.items():
        graph_with_connections[node_id]['connections'] = data
        graph_with_connections[node_id]['members'] = list(clusters[int(node_id)]['members'])
        graph_with_connections[node_id]['center'] = clusters[int(node_id)]['center']

    return graph_with_connections

        

def convert_graph_to_json(averaged_connections):
    json_compatible_graph = {}

    for cluster_id, connections in averaged_connections.items():
        json_compatible_cluster = {}
        for connected_cluster_id, conn_times in connections.items():
            json_compatible_cluster[str(connected_cluster_id)] = {}
            for key, value in conn_times.items():
                # Convert tuple keys to string
                day_type, hour = key
                string_key = f"{day_type}_{hour}"
                json_compatible_cluster[str(connected_cluster_id)][string_key] = value

        json_compatible_graph[str(cluster_id)] = json_compatible_cluster

    return json_compatible_graph



# Djikstra's implementation to determine time for a given trip
def dijkstra(graph, start, end, hour, day_type):
    queue = [(0, start)]  # (cumulative_time, node)
    visited = set()
    # Map to store shortest distance to a node
    distances = {node: float('inf') for node in graph}
    distances[start] = 0


    while queue:
        # Get the node with the smallest cumulative time
        cumulative_time, node = heapq.heappop(queue)
        if node not in visited:
            visited.add(node)

            # Reached destination
            if node == end:
                return cumulative_time

            for neighbor, edges in graph[node]['connections'].items():
                # Filter edges based on day_type and hour
                valid_edges = [edge for edge in edges if edge['day_type'] == day_type and edge['hour'] == hour]

                if not valid_edges:
                    continue

                edge = valid_edges[0]
                travel_time = edge['time'] * 60  # Convert hours to minutes
                new_time = cumulative_time + travel_time

                if new_time < distances[neighbor]:
                    distances[neighbor] = new_time
                    heapq.heappush(queue, (new_time, neighbor))

    # If the destination is not reachable, return infinity
    return float('inf')


# build connections from cluster center to all others
def build_time_dependent_paths(graph, selected_nodes):
    day_types = ['weekday', 'weekend']
    hours = list(range(0,24, 8))  # 0 to 23 with a step of 8
    total_computations = len(selected_nodes) * (len(selected_nodes) - 1) * len(day_types) * len(hours)
    completed_computations = 0

    paths = {node: {day_type: {hour: {} for hour in hours} for day_type in day_types} for node in selected_nodes}

    for start_node in selected_nodes:
        for day_type in day_types:
            for hour in hours:
                for end_node in selected_nodes:
                    if start_node != end_node:
                        if end_node in paths and start_node in paths[end_node][day_type][hour]:
                            path_time = paths[end_node][day_type][hour][start_node]
                        else:
                            path_time = dijkstra(graph, start_node, end_node, hour, day_type)

                        paths[start_node][day_type][hour][end_node] = path_time
                        paths[end_node][day_type][hour][start_node] = path_time

                        completed_computations += 1
                        percentage_completed = (completed_computations / total_computations) * 100
                        if completed_computations % 100 == 0:
                            print(f"Completed: {percentage_completed:.2f}%")

    return paths


def build_clustered_graph(paths, clusters):
    for cluster in clusters:
        clusters[cluster]['connections'] = paths[cluster]
    
    return clusters
    

def build_centers_from_paths(paths, graph):
    for node in paths.keys():
        paths[node]['lat'] = graph[node]['coordinates']['lat']
        paths[node]['lon'] = graph[node]['coordinates']['lon']
    return paths



def knn_wrapper(build_paths=False):
    # Load node data from your JSON file
    print('Loading graph data...')
    with open('graph.json', 'r') as file:
        graph_data = json.load(file)
    

    # Assuming graph_data is a dictionary with node IDs as keys and {'lat': ..., 'lon': ...} as values
    node_data = {node_id: {'lat': graph_data[node_id]['coordinates']['lat'], 'lon': graph_data[node_id]['coordinates']['lon']} for node_id in graph_data}

    print('Initializing cluster centers...')
    selected_nodes = initialize_cluster_centers(node_data)


    if build_paths:
        paths = build_time_dependent_paths(graph_data, selected_nodes)

        paths_file = 'paths.json'
        with open(paths_file, 'w') as f:
            json.dump(paths, f, indent=4)

    with open('paths.json', 'r') as f:
        paths = json.load(f)

    print('Running geospatial clustering...')
    if not build_paths:
        initial_cluster_centers = build_centers_from_paths(paths, graph_data)
    else:
        initial_cluster_centers = [{ node_id: {'lat': node_data[node_id]['lat'], 'lon': node_data[node_id]['lon']}} for node_id in selected_nodes]
        
    clusters = assign_nodes_to_clusters(node_data, initial_cluster_centers, graph_data)

    print(clusters.keys())
    print('Building new graph...')
    clustered_graph = build_clustered_graph(paths, clusters)
    print(clustered_graph )
    
    results_file = 'clustered_graph.json'
    with open(results_file, 'w') as f:
        json.dump(clustered_graph, f, indent=4)


    # cluster_connections = aggregate_cluster_connections(graph_data, clusters)

    # print('Building new graph...')
    # tiny_graph = average_cluster_connections(cluster_connections)

    # print('Refining new graph...')
    # json_graph = convert_graph_to_json(tiny_graph)

    # graph_with_full_metadata = add_cluster_data_to_nodes(json_graph, clusters)

    # results_file = 'tiny_graph.json'

    # with open(results_file, 'w') as f:
    #     json.dump(graph_with_full_metadata, f, indent=4)

if __name__ == "__main__":
    knn_wrapper()






